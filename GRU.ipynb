{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈 ∙ 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "# tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 불러오기 ∙ 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df): #데이터프레임 필터링\n",
    "    # 첫 번째 열에서 같은 값을 가진 행의 수를 계산합니다.\n",
    "    row_counts = df['0'].value_counts()\n",
    "\n",
    "    # 가장 많은 행의 수를 찾습니다.\n",
    "    max_row_count = row_counts.max()\n",
    "\n",
    "    # 가장 많은 행의 수에 해당하는 행만 분류합니다.\n",
    "    filtered = pd.DataFrame(df[df['0'].isin(row_counts[row_counts == max_row_count].index)])\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# 데이터 로드   \n",
    "stock_df = pd.read_csv('/Users/moon/Desktop/Moon SeungHoo/Stock_Machine_Learning/StockData_5%_test.csv',low_memory=False)\n",
    "\n",
    "#데이터 필터링\n",
    "filter_stock = filter_df(stock_df)\n",
    "filter_label = filter_stock['24']\n",
    "\n",
    "# 불필요한 데이터 삭제\n",
    "filter_stock = filter_stock.drop({'0','1','7','24'},axis=1) #날자, 상승율, 5%이상 상승여부 삭제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of zeros (0s): 0.95\n",
      "Ratio of ones (1s): 0.05\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "stock_label = scaler.fit_transform(filter_label.values.reshape(-1, 1))\n",
    "\n",
    "count_zeros = np.sum(stock_label == 0)\n",
    "count_ones = np.sum(stock_label == 1)\n",
    "total_samples = len(stock_label)\n",
    "\n",
    "ratio_zeros = (count_zeros / total_samples).round(2)\n",
    "ratio_ones = (count_ones / total_samples).round(2)\n",
    "\n",
    "print(\"Ratio of zeros (0s):\", ratio_zeros)\n",
    "print(\"Ratio of ones (1s):\", ratio_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of zeros (0s): 0.5\n",
      "Ratio of ones (1s): 0.5\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(filter_stock, stock_label, test_size=0.2, random_state=42)\n",
    "\n",
    "under_sampler = RandomUnderSampler(sampling_strategy='auto' , random_state=42)\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "X_test_resampled, y_test_resampled = under_sampler.fit_resample(X_test, y_test)\n",
    "\n",
    "# smote = SMOTE(sampling_strategy=0.7 ,random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train,y_train)\n",
    "# X_test_resampled, y_test_resampled = smote.fit_resample(X_test,y_test)\n",
    "\n",
    "count_zeros = np.sum(y_resampled == 0)\n",
    "count_ones = np.sum(y_resampled == 1)\n",
    "total_samples = len(y_resampled)\n",
    "\n",
    "ratio_zeros = (count_zeros / total_samples).round(2)\n",
    "ratio_ones = (count_ones / total_samples).round(2)\n",
    "\n",
    "print(\"Ratio of zeros (0s):\", ratio_zeros)\n",
    "print(\"Ratio of ones (1s):\", ratio_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 25, 32)            3360      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 25, 32)            6336      \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54081 (211.25 KB)\n",
      "Trainable params: 54081 (211.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRU 모델 정의\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.GRU(32, input_shape=(X_resampled.shape[1],1),kernel_initializer='glorot_uniform', recurrent_dropout=0.0 ,return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.GRU(32, return_sequences=False))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 조기 종료 콜백 정의\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=30, restore_best_weights=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2587/2587 [==============================] - 99s 36ms/step - loss: 0.4034 - accuracy: 0.8150 - val_loss: 0.3475 - val_accuracy: 0.8491\n",
      "Epoch 2/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2941 - accuracy: 0.8752 - val_loss: 0.3085 - val_accuracy: 0.8717\n",
      "Epoch 3/500\n",
      "2587/2587 [==============================] - 92s 35ms/step - loss: 0.2849 - accuracy: 0.8797 - val_loss: 0.2691 - val_accuracy: 0.8845\n",
      "Epoch 4/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2852 - accuracy: 0.8792 - val_loss: 0.2607 - val_accuracy: 0.8902\n",
      "Epoch 5/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2745 - accuracy: 0.8847 - val_loss: 0.2909 - val_accuracy: 0.8675\n",
      "Epoch 6/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2737 - accuracy: 0.8851 - val_loss: 0.2611 - val_accuracy: 0.8897\n",
      "Epoch 7/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2703 - accuracy: 0.8862 - val_loss: 0.2668 - val_accuracy: 0.8853\n",
      "Epoch 8/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2753 - accuracy: 0.8838 - val_loss: 0.2730 - val_accuracy: 0.8883\n",
      "Epoch 9/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2708 - accuracy: 0.8862 - val_loss: 0.2655 - val_accuracy: 0.8885\n",
      "Epoch 10/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2685 - accuracy: 0.8866 - val_loss: 0.2650 - val_accuracy: 0.8888\n",
      "Epoch 11/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2679 - accuracy: 0.8873 - val_loss: 0.2682 - val_accuracy: 0.8895\n",
      "Epoch 12/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2681 - accuracy: 0.8874 - val_loss: 0.2669 - val_accuracy: 0.8855\n",
      "Epoch 13/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2649 - accuracy: 0.8883 - val_loss: 0.2811 - val_accuracy: 0.8793\n",
      "Epoch 14/500\n",
      "2587/2587 [==============================] - 92s 36ms/step - loss: 0.2633 - accuracy: 0.8888 - val_loss: 0.2619 - val_accuracy: 0.8886\n",
      "Epoch 15/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2603 - accuracy: 0.8906 - val_loss: 0.2667 - val_accuracy: 0.8871\n",
      "Epoch 16/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2602 - accuracy: 0.8904 - val_loss: 0.2634 - val_accuracy: 0.8890\n",
      "Epoch 17/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2581 - accuracy: 0.8920 - val_loss: 0.2513 - val_accuracy: 0.8928\n",
      "Epoch 18/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2579 - accuracy: 0.8913 - val_loss: 0.2555 - val_accuracy: 0.8905\n",
      "Epoch 19/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2580 - accuracy: 0.8924 - val_loss: 0.2558 - val_accuracy: 0.8938\n",
      "Epoch 20/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2571 - accuracy: 0.8920 - val_loss: 0.2610 - val_accuracy: 0.8900\n",
      "Epoch 21/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2560 - accuracy: 0.8916 - val_loss: 0.2862 - val_accuracy: 0.8816\n",
      "Epoch 22/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2544 - accuracy: 0.8925 - val_loss: 0.2546 - val_accuracy: 0.8940\n",
      "Epoch 23/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2545 - accuracy: 0.8927 - val_loss: 0.2558 - val_accuracy: 0.8915\n",
      "Epoch 24/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2532 - accuracy: 0.8925 - val_loss: 0.2536 - val_accuracy: 0.8937\n",
      "Epoch 25/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2525 - accuracy: 0.8938 - val_loss: 0.2614 - val_accuracy: 0.8901\n",
      "Epoch 26/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2508 - accuracy: 0.8940 - val_loss: 0.2552 - val_accuracy: 0.8922\n",
      "Epoch 27/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2491 - accuracy: 0.8955 - val_loss: 0.2556 - val_accuracy: 0.8918\n",
      "Epoch 28/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2494 - accuracy: 0.8954 - val_loss: 0.2633 - val_accuracy: 0.8874\n",
      "Epoch 29/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2492 - accuracy: 0.8956 - val_loss: 0.2560 - val_accuracy: 0.8921\n",
      "Epoch 30/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2491 - accuracy: 0.8954 - val_loss: 0.2576 - val_accuracy: 0.8924\n",
      "Epoch 31/500\n",
      "2587/2587 [==============================] - 92s 35ms/step - loss: 0.2508 - accuracy: 0.8951 - val_loss: 0.2537 - val_accuracy: 0.8943\n",
      "Epoch 32/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2483 - accuracy: 0.8952 - val_loss: 0.2486 - val_accuracy: 0.8954\n",
      "Epoch 33/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2462 - accuracy: 0.8974 - val_loss: 0.2671 - val_accuracy: 0.8851\n",
      "Epoch 34/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2472 - accuracy: 0.8958 - val_loss: 0.2550 - val_accuracy: 0.8933\n",
      "Epoch 35/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2440 - accuracy: 0.8978 - val_loss: 0.2599 - val_accuracy: 0.8906\n",
      "Epoch 36/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2448 - accuracy: 0.8972 - val_loss: 0.2564 - val_accuracy: 0.8905\n",
      "Epoch 37/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2444 - accuracy: 0.8966 - val_loss: 0.2523 - val_accuracy: 0.8959\n",
      "Epoch 38/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2433 - accuracy: 0.8971 - val_loss: 0.2569 - val_accuracy: 0.8916\n",
      "Epoch 39/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2420 - accuracy: 0.8991 - val_loss: 0.2566 - val_accuracy: 0.8939\n",
      "Epoch 40/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2424 - accuracy: 0.8982 - val_loss: 0.2536 - val_accuracy: 0.8960\n",
      "Epoch 41/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2423 - accuracy: 0.8985 - val_loss: 0.2481 - val_accuracy: 0.8964\n",
      "Epoch 42/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2384 - accuracy: 0.9009 - val_loss: 0.2627 - val_accuracy: 0.8920\n",
      "Epoch 43/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2395 - accuracy: 0.8986 - val_loss: 0.2542 - val_accuracy: 0.8970\n",
      "Epoch 44/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2383 - accuracy: 0.9002 - val_loss: 0.2653 - val_accuracy: 0.8912\n",
      "Epoch 45/500\n",
      "2587/2587 [==============================] - 89s 34ms/step - loss: 0.2411 - accuracy: 0.8986 - val_loss: 0.2629 - val_accuracy: 0.8907\n",
      "Epoch 46/500\n",
      "2587/2587 [==============================] - 89s 34ms/step - loss: 0.2388 - accuracy: 0.8994 - val_loss: 0.2534 - val_accuracy: 0.8937\n",
      "Epoch 47/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2379 - accuracy: 0.8991 - val_loss: 0.2588 - val_accuracy: 0.8906\n",
      "Epoch 48/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2379 - accuracy: 0.8999 - val_loss: 0.2594 - val_accuracy: 0.8925\n",
      "Epoch 49/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2371 - accuracy: 0.9002 - val_loss: 0.2618 - val_accuracy: 0.8924\n",
      "Epoch 50/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2394 - accuracy: 0.8987 - val_loss: 0.2547 - val_accuracy: 0.8945\n",
      "Epoch 51/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2371 - accuracy: 0.8993 - val_loss: 0.2741 - val_accuracy: 0.8879\n",
      "Epoch 52/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2351 - accuracy: 0.9008 - val_loss: 0.2552 - val_accuracy: 0.8963\n",
      "Epoch 53/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2354 - accuracy: 0.9017 - val_loss: 0.2608 - val_accuracy: 0.8912\n",
      "Epoch 54/500\n",
      "2587/2587 [==============================] - 92s 35ms/step - loss: 0.2365 - accuracy: 0.9002 - val_loss: 0.2501 - val_accuracy: 0.8966\n",
      "Epoch 55/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2337 - accuracy: 0.9019 - val_loss: 0.2531 - val_accuracy: 0.8949\n",
      "Epoch 56/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2361 - accuracy: 0.9015 - val_loss: 0.2490 - val_accuracy: 0.8984\n",
      "Epoch 57/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2322 - accuracy: 0.9017 - val_loss: 0.2544 - val_accuracy: 0.8931\n",
      "Epoch 58/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2353 - accuracy: 0.8999 - val_loss: 0.2481 - val_accuracy: 0.8976\n",
      "Epoch 59/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2335 - accuracy: 0.9010 - val_loss: 0.2583 - val_accuracy: 0.8931\n",
      "Epoch 60/500\n",
      "2587/2587 [==============================] - 92s 35ms/step - loss: 0.2338 - accuracy: 0.9014 - val_loss: 0.2523 - val_accuracy: 0.8975\n",
      "Epoch 61/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2336 - accuracy: 0.9010 - val_loss: 0.2551 - val_accuracy: 0.8942\n",
      "Epoch 62/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2335 - accuracy: 0.9011 - val_loss: 0.2626 - val_accuracy: 0.8916\n",
      "Epoch 63/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2339 - accuracy: 0.9005 - val_loss: 0.2541 - val_accuracy: 0.8943\n",
      "Epoch 64/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2328 - accuracy: 0.9015 - val_loss: 0.2536 - val_accuracy: 0.8922\n",
      "Epoch 65/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2334 - accuracy: 0.9010 - val_loss: 0.2587 - val_accuracy: 0.8919\n",
      "Epoch 66/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2394 - accuracy: 0.8994 - val_loss: 0.2538 - val_accuracy: 0.8966\n",
      "Epoch 67/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2338 - accuracy: 0.9007 - val_loss: 0.2598 - val_accuracy: 0.8930\n",
      "Epoch 68/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2352 - accuracy: 0.9019 - val_loss: 0.2551 - val_accuracy: 0.8950\n",
      "Epoch 69/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2303 - accuracy: 0.9034 - val_loss: 0.2502 - val_accuracy: 0.8991\n",
      "Epoch 70/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2300 - accuracy: 0.9037 - val_loss: 0.2531 - val_accuracy: 0.8966\n",
      "Epoch 71/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2222 - accuracy: 0.9083 - val_loss: 0.2376 - val_accuracy: 0.9039\n",
      "Epoch 72/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2211 - accuracy: 0.9086 - val_loss: 0.2417 - val_accuracy: 0.9006\n",
      "Epoch 73/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2276 - accuracy: 0.9060 - val_loss: 0.2597 - val_accuracy: 0.8907\n",
      "Epoch 74/500\n",
      "2587/2587 [==============================] - 92s 35ms/step - loss: 0.2324 - accuracy: 0.9013 - val_loss: 0.2578 - val_accuracy: 0.8936\n",
      "Epoch 75/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2448 - accuracy: 0.8968 - val_loss: 0.2552 - val_accuracy: 0.8935\n",
      "Epoch 76/500\n",
      "2587/2587 [==============================] - 92s 36ms/step - loss: 0.2363 - accuracy: 0.9003 - val_loss: 0.2612 - val_accuracy: 0.8898\n",
      "Epoch 77/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2363 - accuracy: 0.9005 - val_loss: 0.2573 - val_accuracy: 0.8933\n",
      "Epoch 78/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2334 - accuracy: 0.9012 - val_loss: 0.2388 - val_accuracy: 0.9019\n",
      "Epoch 79/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2329 - accuracy: 0.9013 - val_loss: 0.2564 - val_accuracy: 0.8950\n",
      "Epoch 80/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2351 - accuracy: 0.9003 - val_loss: 0.2567 - val_accuracy: 0.8942\n",
      "Epoch 81/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2345 - accuracy: 0.9012 - val_loss: 0.2533 - val_accuracy: 0.8985\n",
      "Epoch 82/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2316 - accuracy: 0.9014 - val_loss: 0.2596 - val_accuracy: 0.8938\n",
      "Epoch 83/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2306 - accuracy: 0.9025 - val_loss: 0.2625 - val_accuracy: 0.8929\n",
      "Epoch 84/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2313 - accuracy: 0.9028 - val_loss: 0.2397 - val_accuracy: 0.9015\n",
      "Epoch 85/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2315 - accuracy: 0.9014 - val_loss: 0.2589 - val_accuracy: 0.8939\n",
      "Epoch 86/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2291 - accuracy: 0.9031 - val_loss: 0.2686 - val_accuracy: 0.8927\n",
      "Epoch 87/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2276 - accuracy: 0.9036 - val_loss: 0.2615 - val_accuracy: 0.8929\n",
      "Epoch 88/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2280 - accuracy: 0.9037 - val_loss: 0.2625 - val_accuracy: 0.8937\n",
      "Epoch 89/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2277 - accuracy: 0.9035 - val_loss: 0.2645 - val_accuracy: 0.8933\n",
      "Epoch 90/500\n",
      "2587/2587 [==============================] - 91s 35ms/step - loss: 0.2292 - accuracy: 0.9029 - val_loss: 0.2643 - val_accuracy: 0.8905\n",
      "Epoch 91/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2236 - accuracy: 0.9059 - val_loss: 0.2463 - val_accuracy: 0.8988\n",
      "Epoch 92/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2257 - accuracy: 0.9055 - val_loss: 0.2618 - val_accuracy: 0.8922\n",
      "Epoch 93/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2301 - accuracy: 0.9035 - val_loss: 0.2646 - val_accuracy: 0.8900\n",
      "Epoch 94/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2297 - accuracy: 0.9024 - val_loss: 0.2697 - val_accuracy: 0.8912\n",
      "Epoch 95/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2278 - accuracy: 0.9035 - val_loss: 0.2580 - val_accuracy: 0.8943\n",
      "Epoch 96/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2262 - accuracy: 0.9045 - val_loss: 0.2444 - val_accuracy: 0.9024\n",
      "Epoch 97/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2262 - accuracy: 0.9047 - val_loss: 0.2645 - val_accuracy: 0.8891\n",
      "Epoch 98/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2299 - accuracy: 0.9029 - val_loss: 0.2585 - val_accuracy: 0.8949\n",
      "Epoch 99/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2294 - accuracy: 0.9041 - val_loss: 0.2593 - val_accuracy: 0.8954\n",
      "Epoch 100/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2297 - accuracy: 0.9027 - val_loss: 0.2549 - val_accuracy: 0.8947\n",
      "Epoch 101/500\n",
      "2587/2587 [==============================] - 89s 35ms/step - loss: 0.2303 - accuracy: 0.9030 - val_loss: 0.2675 - val_accuracy: 0.8888\n",
      "Epoch 102/500\n",
      "2587/2587 [==============================] - 90s 35ms/step - loss: 0.2311 - accuracy: 0.9020 - val_loss: 0.2605 - val_accuracy: 0.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moon/anaconda3/envs/ML/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_resampled,y_resampled,epochs=500,\n",
    "validation_data=(X_test_resampled,y_test_resampled),callbacks=[early_stopping])\n",
    " \n",
    "# 모델 저장\n",
    "model.save(\"GRU_Model_9L_32_5%.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6752/6752 [==============================] - 43s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "loaded_model = tf.keras.models.load_model(\"GRU_Model_9L_32_5%.h5\")\n",
    "\n",
    "# 모델을 사용하여 주가 상승 여부 예측\n",
    "test = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8915185824650407\n",
      "Confusion Matrix:\n",
      "[[183115  22487]\n",
      " [   949   9486]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.94    205602\n",
      "         1.0       0.30      0.91      0.45     10435\n",
      "\n",
      "    accuracy                           0.89    216037\n",
      "   macro avg       0.65      0.90      0.69    216037\n",
      "weighted avg       0.96      0.89      0.92    216037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_binary = (test > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "classification_rep = classification_report(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 89.15%\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 이진값(0 또는 1)으로 변환\n",
    "binary_predictions = (test > 0.5).astype(int)\n",
    "\n",
    "# 예측 결과 출력\n",
    "correct_predictions = np.equal(binary_predictions, y_test)  # 정확하게 예측한 경우 True, 그렇지 않으면 False\n",
    "accuracy = np.mean(correct_predictions)  # 정확도 계산\n",
    "\n",
    "print(f\"정확도: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
